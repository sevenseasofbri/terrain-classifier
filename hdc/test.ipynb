{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dataset structure:\n",
    "audio/\n",
    "├── train\n",
    "│  ├── class\n",
    "│  │   ├── 1.wav\n",
    "│  │   ├── 2.wav\n",
    "│  │   ├── ...\n",
    "│  └── ...\n",
    "'''\n",
    "def load_dataset(type=\"train\"):\n",
    "    # Path to audio folder\n",
    "    audio_folder = \"data/{}\".format(type)\n",
    "    # Extract all classes available by reading the subfolder names\n",
    "    classes = sorted(os.listdir(audio_folder))\n",
    "    # Extract all audio files available for each class as a separate list\n",
    "    audio_files = {}\n",
    "    for c in classes:\n",
    "        # Get all files in the class folder\n",
    "        audio_files[c] = sorted(glob(os.path.join(audio_folder, c, \"*.wav\")))\n",
    "\n",
    "    print(\"Classes: \", classes)\n",
    "    print(audio_files)\n",
    "    return audio_files, classes\n",
    "\n",
    "# load_dataset(\"train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zcr(frame, frame_length):\n",
    "    \"\"\"Compute Zero Crossing Rate (ZCR)\"\"\"\n",
    "    # Count the number of times the signal changes sign\n",
    "    zero_crossings = np.sum(np.abs(np.diff(np.sign(frame)))) / 2\n",
    "    return zero_crossings / frame_length\n",
    "    # return librosa.feature.zero_crossing_rate(frame, frame_length=frame_length)[0, 0]\n",
    "\n",
    "def rms(frame):\n",
    "    \"\"\"Compute Root Mean Square (RMS)\"\"\"\n",
    "    # RMS measures the average power of the signal\n",
    "    # return np.sqrt(np.sum(frame**2) / len(frame))\n",
    "    return np.sqrt(np.mean(frame**2))\n",
    "\n",
    "def temporal_entropy(frame):\n",
    "    \"\"\"Compute Temporal Entropy\"\"\"\n",
    "    # Temporal entropy measures the distribution of energy in the time domain\n",
    "    hist = np.histogram(frame, bins=8, range=(np.min(frame), np.max(frame)))[0]\n",
    "    prob = hist / np.sum(hist)\n",
    "    prob = prob[prob > 0]  # Avoid log(0)\n",
    "    return -np.sum(prob * np.log2(prob))\n",
    "\n",
    "def compute_fft(y, frame_length, hop_length):\n",
    "    \"\"\"Compute the Short-Time Fourier Transform (STFT) using NumPy.\"\"\"\n",
    "    # Frame-based processing\n",
    "    # frames = librosa.util.frame(y, frame_length=frame_length, hop_length=hop_length)\n",
    "    # window = np.hanning(frame_length)  # Apply a Hanning window\n",
    "    # fft_result = np.fft.rfft(frames * window[:, None], axis=0)  # Compute FFT\n",
    "    # return np.abs(fft_result)  # Return the magnitude spectrum\n",
    "    return librosa.stft(y, n_fft=frame_length, hop_length=hop_length, window='hann', center=False)\n",
    "\n",
    "def spectral_centroid(S, sr):\n",
    "    \"\"\"Compute Spectral Centroid\"\"\"\n",
    "    # Spectral centroid is the weighted mean of the frequencies\n",
    "    # freqs = np.fft.rfftfreq(S.shape[0] * 2 - 1, d=1/sr)\n",
    "    # magnitude = np.sum(S, axis=1)\n",
    "    # centroid = np.sum(freqs * magnitude) / np.sum(magnitude)\n",
    "    # return centroid\n",
    "    return librosa.feature.spectral_centroid(S=S, sr=sr)\n",
    "\n",
    "def spectral_rolloff(S, sr, roll_percent=0.85):\n",
    "    \"\"\"Compute Spectral Rolloff\"\"\"\n",
    "    # Spectral rolloff is the frequency below which a certain percentage of the total spectral energy is contained\n",
    "    # freqs = np.fft.rfftfreq(S.shape[0] * 2 - 1, d=1/sr)\n",
    "    # total_energy = np.sum(S)\n",
    "    # cumulative_energy = np.cumsum(S)\n",
    "    # rolloff_idx = np.where(cumulative_energy >= roll_percent * total_energy)[0]\n",
    "    # if len(rolloff_idx) == 0:  # Handle case where no index satisfies the condition\n",
    "    #     return freqs[-1]  # Return the highest frequency\n",
    "    # return freqs[rolloff_idx[0]]\n",
    "    # Using librosa's built-in function for simplicity\n",
    "    return librosa.feature.spectral_rolloff(S=S, sr=sr, roll_percent=roll_percent)\n",
    "\n",
    "def spectral_flatness(S):\n",
    "    \"\"\"Compute Spectral Flatness\"\"\"\n",
    "    # Spectral flatness is the ratio of the geometric mean to the arithmetic mean of the spectrum\n",
    "    geometric_mean = np.exp(np.mean(np.log(S + 1e-10)))  # Add small value to avoid log(0)\n",
    "    arithmetic_mean = np.mean(S)\n",
    "    return geometric_mean / arithmetic_mean\n",
    "    # return librosa.feature.spectral_flatness(S=S)\n",
    "\n",
    "def band_ratio(S, sr, frame_length):\n",
    "    \"\"\"Compute Band Energy Ratio (low vs mid frequencies)\"\"\"\n",
    "    # Band energy ratio compares the energy in different frequency bands\n",
    "    # freqs = np.fft.rfftfreq(frame_length, d=1/sr)\n",
    "    freqs = librosa.fft_frequencies(sr=sr, n_fft=frame_length)\n",
    "    low_band = (freqs >= 100) & (freqs < 1000)\n",
    "    mid_band = (freqs >= 1000) & (freqs < 4000)\n",
    "    low_energy = np.sum(S[low_band, :], axis=0)\n",
    "    mid_energy = np.sum(S[mid_band, :], axis=0)\n",
    "    return mid_energy / (low_energy + 1e-10)\n",
    "    # Add small value to avoid division by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2763671875,\n",
       " 0.03773341123035959,\n",
       " 0.0019981977669522167,\n",
       " 0.004150137305259705,\n",
       " 0.9842715292482215,\n",
       " 1786.3950209271018,\n",
       " 2956.43789556962,\n",
       " 0.5788753,\n",
       " 2.9733303]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_audio_features(file_path, sr, frame_length=2048, hop_length=512):\n",
    "    \"\"\"Extract material sound features compatible with HDC requirements\"\"\"\n",
    "    \n",
    "    # Load audio with optimal parameters for material sounds\n",
    "    y, sr = librosa.load(file_path, sr=sr, duration=5.0)  # 16kHz sampling\n",
    "    \n",
    "    # Frame-based processing\n",
    "    frames = librosa.util.frame(y, frame_length=frame_length, hop_length=hop_length)\n",
    "    num_frames = frames.shape[1]\n",
    "    # print(\"Number of frames: \", len(y))\n",
    "    \n",
    "    # Initialize feature arrays with zeros\n",
    "    feature_names = ['zcr', 'rms', 'temporal_entropy', 'spectral_centroid', 'spectral_rolloff', 'spectral_flatness', 'band_ratio']\n",
    "    features = {name: np.zeros(num_frames) for name in feature_names}\n",
    "\n",
    "    # Time-domain features\n",
    "    for i in range(num_frames):\n",
    "        frame = frames[:, i]\n",
    "        features['zcr'][i] = zcr(frame, frame_length)\n",
    "        # print(\"Frame\", i, features['zcr'][i])\n",
    "        features['rms'][i] = rms(frame)\n",
    "        features['temporal_entropy'][i] = temporal_entropy(frame)\n",
    "\n",
    "    # Frequency-domain features\n",
    "    S = np.abs(librosa.stft(y, n_fft=frame_length, hop_length=hop_length))\n",
    "    features['spectral_centroid'] = spectral_centroid(S, sr)\n",
    "    features['spectral_rolloff'] = spectral_rolloff(S, sr)\n",
    "    features['spectral_flatness'] = spectral_flatness(S)\n",
    "\n",
    "    # Band energy ratio (low vs mid frequencies)\n",
    "    features['band_ratio'] = band_ratio(S, sr, frame_length)\n",
    "\n",
    "    # Aggregate statistics for HDC encoding\n",
    "    feature_vector = [\n",
    "        np.mean(features['zcr']), np.std(features['zcr']),\n",
    "        np.mean(features['rms']), np.max(features['rms']),\n",
    "        np.mean(features['temporal_entropy']),\n",
    "        np.mean(features['spectral_centroid']),\n",
    "        np.mean(features['spectral_rolloff']),\n",
    "        np.mean(features['spectral_flatness']),\n",
    "        np.mean(features['band_ratio'])\n",
    "    ]\n",
    "    \n",
    "    return feature_vector\n",
    "\n",
    "# extract_audio_features(\"data/test/metal/artemis_recording_31.wav\", sr=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDC operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature Setup ---\n",
    "feature_names = [\n",
    "    'zcr_mean', 'zcr_std', 'rms_mean', 'rms_max',\n",
    "    'entropy_mean', 'spectral_centroid_mean', 'spectral_rolloff_mean',\n",
    "    'spectral_flatness_mean', 'band_ratio_mean'\n",
    "]\n",
    "\n",
    "# Selectively chosen important feature pairs\n",
    "important_pairs = [\n",
    "    ('zcr_mean', 'entropy_mean'),\n",
    "    ('rms_mean', 'spectral_rolloff_mean'),\n",
    "    ('spectral_flatness_mean', 'spectral_centroid_mean'),\n",
    "    ('rms_max', 'band_ratio_mean')\n",
    "]\n",
    "\n",
    "# --- 1. Feature name codebook (via permutation) ---\n",
    "def generate_feature_codebook(feature_names, D):\n",
    "    base = np.random.randint(0, 2, D, dtype=np.uint8)\n",
    "    return {name: np.roll(base, i + 1) for i, name in enumerate(feature_names)}\n",
    "\n",
    "# --- 2. Pre-generate value level hypervectors ---\n",
    "def generate_value_level_hvs(levels, D):\n",
    "    level_hvs = []\n",
    "    for level in range(levels):\n",
    "        hv = np.zeros(D, dtype=np.uint8)\n",
    "        if level > 0:\n",
    "            n_bits = level * D // levels\n",
    "            indices = np.random.choice(D, n_bits, replace=False)\n",
    "            hv[indices] = 1\n",
    "        level_hvs.append(hv)\n",
    "    return level_hvs\n",
    "\n",
    "# --- 3. Map value to nearest level HV ---\n",
    "def get_value_hv(levels, value, level_hvs):\n",
    "    level = min(levels - 1, max(0, int(value * levels)))\n",
    "    return level_hvs[level]\n",
    "\n",
    "# --- 4. Encode single audio feature vector ---\n",
    "def encode_feature_vector(features, codebook, level_hvs, D, levels):\n",
    "    assert len(features) == len(codebook)\n",
    "    \n",
    "    feature_dict = dict(zip(codebook.keys(), features))\n",
    "    hvs = []\n",
    "\n",
    "    # Step 1: Encode individual features (key ⊙ value)\n",
    "    for name, value in feature_dict.items():\n",
    "        feat_hv = np.bitwise_xor(codebook[name], get_value_hv(levels, value, level_hvs))\n",
    "        hvs.append(feat_hv.astype(np.int16))\n",
    "\n",
    "    # Step 2: Encode selected feature-pair interactions (bound pair HVs)\n",
    "    for f1, f2 in important_pairs:\n",
    "        hv1 = np.bitwise_xor(codebook[f1], get_value_hv(levels, feature_dict[f1], level_hvs))\n",
    "        hv2 = np.bitwise_xor(codebook[f2], get_value_hv(levels, feature_dict[f2], level_hvs))\n",
    "        pair_hv = np.bitwise_xor(hv1, hv2)\n",
    "        hvs.append(pair_hv.astype(np.int16))\n",
    "\n",
    "    # Optional: visualize before bundling\n",
    "    # plt.figure(figsize=(10, 8))\n",
    "    # plt.imshow(hvs, aspect='auto', cmap='hot', vmin=0, vmax=1)\n",
    "    # plt.colorbar()\n",
    "    \n",
    "    # Step 3: Final bundling (majority vote)\n",
    "    hvs = np.array(hvs)\n",
    "    sum_hv = np.sum(hvs, axis=0)\n",
    "    threshold = len(hvs) // 2\n",
    "    final_hv = (sum_hv > threshold).astype(np.uint8)\n",
    "    # print(sum_hv.shape, hvs.shape, len(hvs),final_hv.shape)\n",
    "\n",
    "    return final_hv\n",
    "\n",
    "#######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hd_classifier(dataset, labels, codebook, level_hvs, D, levels, epochs):\n",
    "    \"\"\"\n",
    "    dataset: list or np.array of normalized feature vectors (N x 9)\n",
    "    labels: list or np.array of corresponding class labels (N)\n",
    "    codebook: feature-name -> HVs (symbolic keys of 9 features)\n",
    "    level_hvs: list of pre-generated value level HVs\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = np.copy(dataset)\n",
    "    labels = np.copy(labels)\n",
    "\n",
    "    num_classes = len(np.unique(labels))\n",
    "    real_class_hvs = np.zeros((num_classes, D), dtype=np.int16)\n",
    "\n",
    "    N = len(dataset)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(N):\n",
    "            query_hv = dataset[i]\n",
    "            y_true = labels[i]\n",
    "\n",
    "            # Binarize class HVs\n",
    "            bin_class_hvs = (real_class_hvs >= 0).astype(np.uint8)\n",
    "\n",
    "            # Predict using Hamming distance\n",
    "            predictions = np.sum(query_hv != bin_class_hvs, axis=1)\n",
    "            y_pred = np.argmin(predictions)\n",
    "\n",
    "            # OnlineHD-style update\n",
    "            if y_pred != y_true:\n",
    "                real_class_hvs[y_true] += query_hv\n",
    "                real_class_hvs[y_pred] -= query_hv\n",
    "\n",
    "        # Shuffle\n",
    "        indices = np.random.permutation(N)\n",
    "        dataset = dataset[indices]\n",
    "        labels = labels[indices]\n",
    "\n",
    "    final_class_hvs = np.zeros((num_classes, D), dtype=np.int16)\n",
    "    final_class_hvs = (real_class_hvs >= 0).astype(np.uint8)\n",
    "    return final_class_hvs\n",
    "\n",
    "def predict_hd(query_hv, class_hvs):\n",
    "    \"\"\"\n",
    "    Predict label for a query hypervector using Hamming distance.\n",
    "    \"\"\"\n",
    "    distances = [np.sum(query_hv != class_hv) for class_hv in class_hvs]\n",
    "    return np.argmin(distances)\n",
    "\n",
    "def evaluate_hd(vectors, labels, class_hvs):\n",
    "    correct = 0\n",
    "    for query_hv, y_true in zip(vectors, labels, strict=True):\n",
    "\n",
    "        # query_hv = encode_feature_vector(x, codebook, level_hvs, D, levels)\n",
    "\n",
    "        y_pred = predict_hd(query_hv, class_hvs)\n",
    "        if y_pred == y_true:\n",
    "            correct += 1\n",
    "        # else:\n",
    "            # print(f\"Predicted: {y_pred}, True: {y_true}\")\n",
    "    return correct / len(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes:  ['cloth', 'grass', 'metal', 'sandpaper', 'wood']\n",
      "{'cloth': ['data/train/cloth/artemis_recording_01.wav', 'data/train/cloth/artemis_recording_02.wav', 'data/train/cloth/artemis_recording_03.wav', 'data/train/cloth/artemis_recording_04.wav', 'data/train/cloth/artemis_recording_05.wav', 'data/train/cloth/artemis_recording_06.wav', 'data/train/cloth/artemis_recording_07.wav', 'data/train/cloth/artemis_recording_08.wav', 'data/train/cloth/artemis_recording_09.wav', 'data/train/cloth/artemis_recording_10.wav', 'data/train/cloth/artemis_recording_11.wav', 'data/train/cloth/artemis_recording_12.wav', 'data/train/cloth/artemis_recording_13.wav', 'data/train/cloth/artemis_recording_14.wav', 'data/train/cloth/artemis_recording_15.wav', 'data/train/cloth/artemis_recording_16.wav', 'data/train/cloth/artemis_recording_17.wav', 'data/train/cloth/artemis_recording_18.wav', 'data/train/cloth/artemis_recording_19.wav', 'data/train/cloth/artemis_recording_20.wav', 'data/train/cloth/artemis_recording_21.wav', 'data/train/cloth/artemis_recording_22.wav', 'data/train/cloth/artemis_recording_23.wav', 'data/train/cloth/artemis_recording_24.wav', 'data/train/cloth/artemis_recording_25.wav', 'data/train/cloth/artemis_recording_26.wav', 'data/train/cloth/artemis_recording_27.wav', 'data/train/cloth/artemis_recording_28.wav', 'data/train/cloth/artemis_recording_29.wav', 'data/train/cloth/artemis_recording_30.wav'], 'grass': ['data/train/grass/artemis_recording_01.wav', 'data/train/grass/artemis_recording_02.wav', 'data/train/grass/artemis_recording_03.wav', 'data/train/grass/artemis_recording_04.wav', 'data/train/grass/artemis_recording_05.wav', 'data/train/grass/artemis_recording_06.wav', 'data/train/grass/artemis_recording_07.wav', 'data/train/grass/artemis_recording_08.wav', 'data/train/grass/artemis_recording_09.wav', 'data/train/grass/artemis_recording_10.wav', 'data/train/grass/artemis_recording_11.wav', 'data/train/grass/artemis_recording_12.wav', 'data/train/grass/artemis_recording_13.wav', 'data/train/grass/artemis_recording_14.wav', 'data/train/grass/artemis_recording_15.wav', 'data/train/grass/artemis_recording_16.wav', 'data/train/grass/artemis_recording_17.wav', 'data/train/grass/artemis_recording_18.wav', 'data/train/grass/artemis_recording_19.wav', 'data/train/grass/artemis_recording_20.wav', 'data/train/grass/artemis_recording_21.wav', 'data/train/grass/artemis_recording_22.wav', 'data/train/grass/artemis_recording_23.wav', 'data/train/grass/artemis_recording_24.wav', 'data/train/grass/artemis_recording_25.wav', 'data/train/grass/artemis_recording_26.wav', 'data/train/grass/artemis_recording_27.wav', 'data/train/grass/artemis_recording_28.wav', 'data/train/grass/artemis_recording_29.wav', 'data/train/grass/artemis_recording_30.wav'], 'metal': ['data/train/metal/artemis_recording_01.wav', 'data/train/metal/artemis_recording_02.wav', 'data/train/metal/artemis_recording_03.wav', 'data/train/metal/artemis_recording_04.wav', 'data/train/metal/artemis_recording_05.wav', 'data/train/metal/artemis_recording_06.wav', 'data/train/metal/artemis_recording_07.wav', 'data/train/metal/artemis_recording_08.wav', 'data/train/metal/artemis_recording_09.wav', 'data/train/metal/artemis_recording_10.wav', 'data/train/metal/artemis_recording_11.wav', 'data/train/metal/artemis_recording_12.wav', 'data/train/metal/artemis_recording_13.wav', 'data/train/metal/artemis_recording_14.wav', 'data/train/metal/artemis_recording_15.wav', 'data/train/metal/artemis_recording_16.wav', 'data/train/metal/artemis_recording_17.wav', 'data/train/metal/artemis_recording_18.wav', 'data/train/metal/artemis_recording_19.wav', 'data/train/metal/artemis_recording_20.wav', 'data/train/metal/artemis_recording_21.wav', 'data/train/metal/artemis_recording_22.wav', 'data/train/metal/artemis_recording_23.wav', 'data/train/metal/artemis_recording_24.wav', 'data/train/metal/artemis_recording_25.wav', 'data/train/metal/artemis_recording_26.wav', 'data/train/metal/artemis_recording_27.wav', 'data/train/metal/artemis_recording_28.wav', 'data/train/metal/artemis_recording_29.wav', 'data/train/metal/artemis_recording_30.wav'], 'sandpaper': ['data/train/sandpaper/artemis_recording_01.wav', 'data/train/sandpaper/artemis_recording_02.wav', 'data/train/sandpaper/artemis_recording_03.wav', 'data/train/sandpaper/artemis_recording_04.wav', 'data/train/sandpaper/artemis_recording_05.wav', 'data/train/sandpaper/artemis_recording_06.wav', 'data/train/sandpaper/artemis_recording_07.wav', 'data/train/sandpaper/artemis_recording_08.wav', 'data/train/sandpaper/artemis_recording_09.wav', 'data/train/sandpaper/artemis_recording_10.wav', 'data/train/sandpaper/artemis_recording_11.wav', 'data/train/sandpaper/artemis_recording_12.wav', 'data/train/sandpaper/artemis_recording_13.wav', 'data/train/sandpaper/artemis_recording_14.wav', 'data/train/sandpaper/artemis_recording_15.wav', 'data/train/sandpaper/artemis_recording_16.wav', 'data/train/sandpaper/artemis_recording_17.wav', 'data/train/sandpaper/artemis_recording_18.wav', 'data/train/sandpaper/artemis_recording_19.wav', 'data/train/sandpaper/artemis_recording_20.wav', 'data/train/sandpaper/artemis_recording_21.wav', 'data/train/sandpaper/artemis_recording_22.wav', 'data/train/sandpaper/artemis_recording_23.wav', 'data/train/sandpaper/artemis_recording_24.wav', 'data/train/sandpaper/artemis_recording_25.wav', 'data/train/sandpaper/artemis_recording_26.wav', 'data/train/sandpaper/artemis_recording_27.wav', 'data/train/sandpaper/artemis_recording_28.wav', 'data/train/sandpaper/artemis_recording_29.wav', 'data/train/sandpaper/artemis_recording_30.wav'], 'wood': ['data/train/wood/artemis_recording_01.wav', 'data/train/wood/artemis_recording_02.wav', 'data/train/wood/artemis_recording_03.wav', 'data/train/wood/artemis_recording_04.wav', 'data/train/wood/artemis_recording_05.wav', 'data/train/wood/artemis_recording_06.wav', 'data/train/wood/artemis_recording_07.wav', 'data/train/wood/artemis_recording_08.wav', 'data/train/wood/artemis_recording_09.wav', 'data/train/wood/artemis_recording_10.wav', 'data/train/wood/artemis_recording_11.wav', 'data/train/wood/artemis_recording_12.wav', 'data/train/wood/artemis_recording_13.wav', 'data/train/wood/artemis_recording_14.wav', 'data/train/wood/artemis_recording_15.wav', 'data/train/wood/artemis_recording_16.wav', 'data/train/wood/artemis_recording_17.wav', 'data/train/wood/artemis_recording_18.wav', 'data/train/wood/artemis_recording_19.wav', 'data/train/wood/artemis_recording_20.wav', 'data/train/wood/artemis_recording_21.wav', 'data/train/wood/artemis_recording_22.wav', 'data/train/wood/artemis_recording_23.wav', 'data/train/wood/artemis_recording_24.wav', 'data/train/wood/artemis_recording_25.wav', 'data/train/wood/artemis_recording_26.wav', 'data/train/wood/artemis_recording_27.wav', 'data/train/wood/artemis_recording_28.wav', 'data/train/wood/artemis_recording_29.wav', 'data/train/wood/artemis_recording_30.wav']}\n",
      "Training accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# --- Main execution ---\n",
    "\n",
    "# Load dataset\n",
    "dataset = [] # audio features\n",
    "labels = [] # class ids\n",
    "\n",
    "audio_files, classes = load_dataset(\"train\")\n",
    "\n",
    "# Prepare a dictionary of labels and their IDs\n",
    "label_to_id = {class_name: idx for idx, class_name in enumerate(classes)}\n",
    "id_to_label = {idx: class_name for class_name, idx in label_to_id.items()}\n",
    "\n",
    "# Extract features\n",
    "for _, class_name in enumerate(classes):\n",
    "    for file_path in audio_files[class_name]:\n",
    "        features = extract_audio_features(file_path, sr=8000)\n",
    "        dataset.append(features)\n",
    "        labels.append(label_to_id[class_name])\n",
    "\n",
    "\n",
    "# Normalize feature-wise (0–1 scaling)\n",
    "# dataset = np.array(dataset)\n",
    "# dataset = (dataset - dataset.min(axis=0)) / (dataset.max(axis=0) - dataset.min(axis=0))\n",
    "\n",
    "# generate codebooks\n",
    "D = 250  # Hypervector dimensionality\n",
    "LEVELS = 128 # quantization levels\n",
    "np.random.seed(42)\n",
    "codebook = generate_feature_codebook(feature_names, D)\n",
    "value_level_hvs = generate_value_level_hvs(LEVELS, D)\n",
    "\n",
    "vectors = [] # encode dataset vectors\n",
    "\n",
    "for features in dataset:\n",
    "    encoded_hv = encode_feature_vector(features, codebook, value_level_hvs, D, LEVELS)\n",
    "    vectors.append(encoded_hv)\n",
    "\n",
    "vectors = np.array(vectors)\n",
    "labels = np.array(labels)\n",
    "\n",
    "\n",
    "# create weights folder if it doesn't exist\n",
    "if not os.path.exists(\"weights\"):\n",
    "    os.makedirs(\"weights\")\n",
    "\n",
    "# for i in range(20):\n",
    "    # train the classifier\n",
    "class_hvs = train_hd_classifier(vectors, labels, codebook, value_level_hvs, D, LEVELS, epochs=20)\n",
    "\n",
    "# # convert class_hvs to boolean array\n",
    "# class_hvs = (class_hvs >= 0).astype(np.dtypes.BoolDType())\n",
    "# # Save the trained model\n",
    "np.save(\"weights/class_hvs.npy\", class_hvs) # save class hypervectors\n",
    "np.save(\"weights/codebook.npy\", codebook) # save codebook\n",
    "np.save(\"weights/value_level_hvs.npy\", value_level_hvs) # save value level hypervectors\n",
    "np.save(\"weights/label_to_id.npy\", label_to_id) # save label_to_id mapping\n",
    "\n",
    "acc = evaluate_hd(vectors, labels, class_hvs)\n",
    "print(f\"Training accuracy: {acc * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes:  ['cloth', 'grass', 'metal', 'sandpaper', 'wood']\n",
      "{'cloth': ['data/test/cloth/artemis_recording_31.wav', 'data/test/cloth/artemis_recording_32.wav', 'data/test/cloth/artemis_recording_33.wav', 'data/test/cloth/artemis_recording_34.wav', 'data/test/cloth/artemis_recording_35.wav'], 'grass': ['data/test/grass/artemis_recording_31.wav', 'data/test/grass/artemis_recording_32.wav', 'data/test/grass/artemis_recording_33.wav', 'data/test/grass/artemis_recording_34.wav', 'data/test/grass/artemis_recording_35.wav'], 'metal': ['data/test/metal/artemis_recording_31.wav', 'data/test/metal/artemis_recording_32.wav', 'data/test/metal/artemis_recording_33.wav', 'data/test/metal/artemis_recording_34.wav', 'data/test/metal/artemis_recording_35.wav'], 'sandpaper': ['data/test/sandpaper/artemis_recording_31.wav', 'data/test/sandpaper/artemis_recording_32.wav', 'data/test/sandpaper/artemis_recording_33.wav', 'data/test/sandpaper/artemis_recording_34.wav', 'data/test/sandpaper/artemis_recording_35.wav', 'data/test/sandpaper/artemis_recording_36.wav', 'data/test/sandpaper/artemis_recording_37.wav', 'data/test/sandpaper/artemis_recording_38.wav', 'data/test/sandpaper/artemis_recording_39.wav', 'data/test/sandpaper/artemis_recording_40.wav'], 'wood': ['data/test/wood/artemis_recording_31.wav', 'data/test/wood/artemis_recording_32.wav', 'data/test/wood/artemis_recording_33.wav', 'data/test/wood/artemis_recording_34.wav', 'data/test/wood/artemis_recording_35.wav']}\n",
      "Testing accuracy: 93.33%\n"
     ]
    }
   ],
   "source": [
    "# --- Main execution ---\n",
    "\n",
    "# Load dataset\n",
    "dataset = [] # audio features\n",
    "labels = [] # class ids\n",
    "\n",
    "audio_files, classes = load_dataset(\"test\")\n",
    "\n",
    "# Prepare a dictionary of labels and their IDs\n",
    "label_to_id = np.load(\"weights/label_to_id.npy\", allow_pickle=True).item()\n",
    "\n",
    "# Extract features\n",
    "for label_idx, class_name in enumerate(classes):\n",
    "    for file_path in audio_files[class_name]:\n",
    "        features = extract_audio_features(file_path, sr=8000)\n",
    "        dataset.append(features)\n",
    "        labels.append(label_to_id[class_name])\n",
    "\n",
    "\n",
    "# Normalize feature-wise (0–1 scaling)\n",
    "# dataset = np.array(dataset)\n",
    "# dataset = (dataset - dataset.min(axis=0)) / (dataset.max(axis=0) - dataset.min(axis=0))\n",
    "\n",
    "# generate codebooks\n",
    "# D = 10000  # Hypervector dimensionality\n",
    "# LEVELS = 256 # quantization levels\n",
    "np.random.seed(42)\n",
    "codebook = np.load(\"weights/codebook.npy\", allow_pickle=True).item()\n",
    "value_level_hvs = np.load(\"weights/value_level_hvs.npy\", allow_pickle=True)\n",
    "\n",
    "vectors = [] # encode dataset vectors\n",
    "\n",
    "for features in dataset:\n",
    "    encoded_hv = encode_feature_vector(features, codebook, value_level_hvs, D, LEVELS)\n",
    "    vectors.append(encoded_hv)\n",
    "\n",
    "vectors = np.array(vectors)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# train the trained class hypervectors\n",
    "class_hvs = np.load(\"weights/class_hvs.npy\", allow_pickle=True)\n",
    "\n",
    "# Evaluate training performance\n",
    "acc = evaluate_hd(vectors, labels, class_hvs)\n",
    "print(f\"Testing accuracy: {acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export weights to C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save class_hvs, codebook, value_level_hvs, label_to_id \n",
    "# to a C header file\n",
    "def save_to_c_header(class_hvs, codebook, value_level_hvs, label_to_id):\n",
    "    with open(\"weights.h\", \"w\") as f:\n",
    "        f.write(\"#ifndef HD_WEIGHTS_H\\n\")\n",
    "        f.write(\"#define HD_WEIGHTS_H\\n\\n\")\n",
    "        f.write(\"#include <stdint.h>\\n\\n\")\n",
    "        \n",
    "        # Define constants\n",
    "        f.write(\"#define D {}\\n\".format(len(class_hvs[0])))\n",
    "        f.write(\"#define NUM_CLASSES {}\\n\".format(len(class_hvs)))\n",
    "        f.write(\"#define NUM_FEATURES {}\\n\".format(len(codebook)))\n",
    "        f.write(\"#define LEVELS {}\\n\\n\".format(len(value_level_hvs)))\n",
    "\n",
    "        # Save class hypervectors\n",
    "        f.write(\"const uint8_t class_hvs[NUM_CLASSES][D] = {\\n\")\n",
    "        for hv in class_hvs:\n",
    "            f.write(\"  {\")\n",
    "            f.write(\", \".join(map(str, hv)))\n",
    "            f.write(\"},\\n\")\n",
    "        f.write(\"};\\n\\n\")\n",
    "\n",
    "        # Save codebook\n",
    "        f.write(\"const uint8_t codebook[NUM_FEATURES][D] = {\\n\")\n",
    "        features = list(codebook.keys())\n",
    "        for feature_name in features:\n",
    "            hv = codebook[feature_name]\n",
    "            f.write(\"  {\")\n",
    "            f.write(\", \".join(map(str, hv)))\n",
    "            f.write(\"}}, // {}\\n\".format(feature_name))\n",
    "        f.write(\"};\\n\\n\")\n",
    "\n",
    "        # Save value level hypervectors\n",
    "        f.write(\"const uint8_t value_level_hvs[LEVELS][D] = {\\n\")\n",
    "        for hv in value_level_hvs:\n",
    "            f.write(\"  {\")\n",
    "            f.write(\", \".join(map(str, hv)))\n",
    "            f.write(\"},\\n\")\n",
    "        f.write(\"};\\n\\n\")\n",
    "\n",
    "        # Save label to ID mapping\n",
    "        f.write(\"const char* label_names[NUM_CLASSES] = {\\n\")\n",
    "        for label_name, idx in sorted(label_to_id.items(), key=lambda x: x[1]):\n",
    "            f.write('  \"{}\",\\n'.format(label_name))\n",
    "        f.write(\"};\\n\\n\")\n",
    "\n",
    "        f.write(\"#endif // HD_WEIGHTS_H\\n\")\n",
    "\n",
    "# Save to C header file\n",
    "save_to_c_header(class_hvs, codebook, value_level_hvs, label_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import sys\n",
    "\n",
    "def convert_to_pcm16(input_file, output_file, target_sr=8000):\n",
    "    # Load audio using librosa, forcing mono and resampling as needed.\n",
    "    # The samples will be in float32 format in range [-1, 1]\n",
    "    y, sr = librosa.load(input_file, sr=target_sr, duration=5.0)\n",
    "    # print(len(y), y[:10])\n",
    "    # save the output y into a c header file \n",
    "    with open(\"audio_data.h\", \"w\") as f:\n",
    "        f.write(\"#ifndef AUDIO_DATA_H\\n\")\n",
    "        f.write(\"#define AUDIO_DATA_H\\n\\n\")\n",
    "        f.write(\"#include <stdint.h>\\n\\n\")\n",
    "        f.write(f\"#define AUDIO_LENGTH {len(y)}\\n\\n\")\n",
    "        f.write(\"const float audio_data_vector[AUDIO_LENGTH] = {\\n\")\n",
    "        f.write(\",\\n\".join(f\"    {sample:.6f}\" for sample in y))\n",
    "        f.write(\"\\n};\\n\\n\")\n",
    "        f.write(\"#endif // AUDIO_DATA_H\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # if len(sys.argv) != 3:\n",
    "    #     print(\"Usage: python convert_to_pcm16.py <input_audio_file> <output_wav_file>\")\n",
    "    # else:\n",
    "    # input_file = sys.argv[1]\n",
    "    # output_file = sys.argv[2]\n",
    "    convert_to_pcm16(\"data/test/sandpaper/artemis_recording_33.wav\", \"converted.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2797721354166667,\n",
       " 0.045330173831118946,\n",
       " 0.0020035209568838278,\n",
       " 0.00376133993268013,\n",
       " 0.9683661591924575,\n",
       " 1782.7016346292512,\n",
       " 2965.7337816455697,\n",
       " 0.58610415,\n",
       " 2.9254842]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_audio_features(\"data/test/metal/artemis_recording_33.wav\", sr=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import torch\n",
    "import torchaudio.transforms as T\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import tempfile\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Step 1: Load dataset\n",
    "def load_dataset(type=\"train\"):\n",
    "    audio_folder = f\"data/{type}\"\n",
    "    classes = sorted(os.listdir(audio_folder))\n",
    "    audio_files = {c: sorted(glob(os.path.join(audio_folder, c, \"*.wav\"))) for c in classes}\n",
    "    return audio_files, classes\n",
    "\n",
    "# Step 2: Extract features\n",
    "def extract_features(file_path, transform, max_len=250):\n",
    "    waveform, sr = torchaudio.load(file_path)\n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "    mfcc = transform(waveform).squeeze(0).transpose(0, 1)\n",
    "    if mfcc.shape[0] < max_len:\n",
    "        pad = torch.zeros(max_len - mfcc.shape[0], mfcc.shape[1])\n",
    "        mfcc = torch.cat((mfcc, pad), dim=0)\n",
    "    else:\n",
    "        mfcc = mfcc[:max_len, :]\n",
    "    return mfcc.flatten().numpy()\n",
    "\n",
    "# Step 3: ML and DNN benchmarking\n",
    "def model_size_mb(model):\n",
    "    with tempfile.NamedTemporaryFile(delete=True) as f:\n",
    "        pickle.dump(model, f)\n",
    "        return os.path.getsize(f.name) / (1024 * 1024)\n",
    "\n",
    "# Step 4: Torch NN Models\n",
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "class TinyCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(8 * 13 * 125, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # [B, 8, 13, 125]\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc1(x)\n",
    "\n",
    "# Run everything\n",
    "audio_files, classes = load_dataset(\"train\")\n",
    "label_to_id = {name: i for i, name in enumerate(classes)}\n",
    "id_to_label = {i: name for name, i in label_to_id.items()}\n",
    "\n",
    "transform = T.MFCC(sample_rate=16000, n_mfcc=13, melkwargs={\"n_fft\": 400, \"hop_length\": 160, \"n_mels\": 23})\n",
    "\n",
    "X, y = [], []\n",
    "for label, files in audio_files.items():\n",
    "    for f in files:\n",
    "        X.append(extract_features(f, transform))\n",
    "        y.append(label_to_id[label])\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ML models\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y)\n",
    "results = {}\n",
    "\n",
    "for name, model in {\n",
    "    \"SVM\": SVC(),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=100)\n",
    "}.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    acc = model.score(X_test, y_test)\n",
    "    size_mb = model_size_mb(model)\n",
    "    results[name] = {\"accuracy\": acc, \"model_size_MB\": size_mb}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "\n",
    "# DNN models\n",
    "X_train_torch = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_torch = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_torch = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_torch, y_train_torch), batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test_torch, y_test_torch), batch_size=16)\n",
    "\n",
    "ffnn = FeedforwardNN(X_train.shape[1], len(classes))\n",
    "optimizer = torch.optim.Adam(ffnn.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(ffnn(xb), yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Accuracy\n",
    "ffnn.eval()\n",
    "with torch.no_grad():\n",
    "    acc = (ffnn(X_test_torch).argmax(1) == y_test_torch).float().mean().item()\n",
    "    torch.save(ffnn.state_dict(), \"ffnn_model.pt\")\n",
    "    size_mb = os.path.getsize(\"ffnn_model.pt\") / (1024 * 1024)\n",
    "    results[\"FeedforwardNN\"] = {\"accuracy\": acc, \"model_size_MB\": size_mb}\n",
    "\n",
    "# TinyCNN using reshaped MFCC\n",
    "# Reshape X for CNN: [batch, 1, 250, 13] → transpose last two dims\n",
    "X_cnn = X.reshape(-1, 250, 13)\n",
    "X_cnn = np.transpose(X_cnn, (0, 2, 1))  # [B, 13, 250]\n",
    "X_cnn = X_cnn[:, :, :250]  # Ensure fixed length\n",
    "X_cnn = X_cnn[:, :, ::2]   # Downsample time axis for TinyCNN \n",
    "\n",
    "# Update the CNN class to match the actual shape of the data\n",
    "class TinyCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        # Calculate the actual flattened size based on our data dimensions\n",
    "        self.fc1 = nn.Linear(8 * 6 * 62, num_classes)  # 13/2 (pool) = ~6, 250/2/2 = ~62\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return self.fc1(x)\n",
    "\n",
    "X_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(X_cnn, y, test_size=0.2, stratify=y)\n",
    "X_train_cnn = torch.tensor(X_train_cnn[:, np.newaxis, :, :], dtype=torch.float32)\n",
    "X_test_cnn = torch.tensor(X_test_cnn[:, np.newaxis, :, :], dtype=torch.float32)\n",
    "y_train_cnn = torch.tensor(y_train_cnn, dtype=torch.long)\n",
    "y_test_cnn = torch.tensor(y_test_cnn, dtype=torch.long)\n",
    "\n",
    "train_loader_cnn = DataLoader(TensorDataset(X_train_cnn, y_train_cnn), batch_size=16, shuffle=True)\n",
    "cnn = TinyCNN(num_classes=len(classes))\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for xb, yb in train_loader_cnn:\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(cnn(xb), yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "cnn.eval()\n",
    "with torch.no_grad():\n",
    "    acc = (cnn(X_test_cnn).argmax(1) == y_test_cnn).float().mean().item()\n",
    "    torch.save(cnn.state_dict(), \"tinycnn_model.pt\")\n",
    "    size_mb = os.path.getsize(\"tinycnn_model.pt\") / (1024 * 1024)\n",
    "    results[\"TinyCNN\"] = {\"accuracy\": acc, \"model_size_MB\": size_mb}\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# === LOAD AND EVALUATE ON TEST DATA ===\n",
    "\n",
    "# Load test data\n",
    "test_audio_files, _ = load_dataset(\"test\")\n",
    "X_test_real, y_test_real = [], []\n",
    "for label, files in test_audio_files.items():\n",
    "    for f in files:\n",
    "        X_test_real.append(extract_features(f, transform))\n",
    "        y_test_real.append(label_to_id[label])\n",
    "X_test_real = np.array(X_test_real)\n",
    "y_test_real = np.array(y_test_real)\n",
    "\n",
    "# === Apply same scaler ===\n",
    "X_test_real_scaled = scaler.transform(X_test_real)\n",
    "\n",
    "# === Evaluate SVM and RF ===\n",
    "results_test = {}\n",
    "for name, model in {\n",
    "    \"SVM\": SVC(),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=100)\n",
    "}.items():\n",
    "    model.fit(X_scaled, y)  # train on full training set\n",
    "    acc = model.score(X_test_real_scaled, y_test_real)\n",
    "    results_test[name] = {\"test_accuracy\": acc}\n",
    "\n",
    "# === Evaluate Feedforward NN ===\n",
    "X_test_real_torch = torch.tensor(X_test_real_scaled, dtype=torch.float32)\n",
    "y_test_real_torch = torch.tensor(y_test_real, dtype=torch.long)\n",
    "\n",
    "ffnn.eval()\n",
    "with torch.no_grad():\n",
    "    acc = (ffnn(X_test_real_torch).argmax(1) == y_test_real_torch).float().mean().item()\n",
    "    results_test[\"FeedforwardNN\"] = {\"test_accuracy\": acc}\n",
    "\n",
    "# === Evaluate TinyCNN ===\n",
    "X_test_real_cnn = X_test_real.reshape(-1, 250, 13)\n",
    "X_test_real_cnn = np.transpose(X_test_real_cnn, (0, 2, 1))[:, :, :250]\n",
    "X_test_real_cnn = X_test_real_cnn[:, :, ::2]  # Downsample\n",
    "\n",
    "X_test_real_cnn_tensor = torch.tensor(X_test_real_cnn[:, np.newaxis, :, :], dtype=torch.float32)\n",
    "y_test_real_cnn_tensor = torch.tensor(y_test_real, dtype=torch.long)\n",
    "\n",
    "cnn.eval()\n",
    "with torch.no_grad():\n",
    "    acc = (cnn(X_test_real_cnn_tensor).argmax(1) == y_test_real_cnn_tensor).float().mean().item()\n",
    "    results_test[\"TinyCNN\"] = {\"test_accuracy\": acc}\n",
    "\n",
    "print(\"\\n=== FINAL TEST ACCURACIES ===\")\n",
    "pprint.pprint(results_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query HV is compared to all the class HVs in the item memory using Hamming distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy vs Epoch for different D values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# --- Main execution ---\n",
    "\n",
    "# Load dataset\n",
    "dataset = [] # audio features\n",
    "labels = [] # class ids\n",
    "\n",
    "audio_files, classes = load_dataset(\"train\")\n",
    "\n",
    "# Prepare a dictionary of labels and their IDs\n",
    "label_to_id = {class_name: idx for idx, class_name in enumerate(classes)}\n",
    "id_to_label = {idx: class_name for class_name, idx in label_to_id.items()}\n",
    "\n",
    "# Extract features\n",
    "for _, class_name in enumerate(classes):\n",
    "    for file_path in audio_files[class_name]:\n",
    "        features = extract_audio_features(file_path, sr=8000)\n",
    "        dataset.append(features)\n",
    "        labels.append(label_to_id[class_name])\n",
    "\n",
    "\n",
    "# Normalize feature-wise (0–1 scaling)\n",
    "dataset = np.array(dataset)\n",
    "dataset = (dataset - dataset.min(axis=0)) / (dataset.max(axis=0) - dataset.min(axis=0))\n",
    "\n",
    "# generate codebooks\n",
    "D = 100  # Hypervector dimensionality\n",
    "LEVELS = 256 # quantization levels\n",
    "\n",
    "D_values = [2**2, 2**4, 2**6, 2**8] # levels\n",
    "# D_values = [2**6, 2**8, 2**10, 2**12]\n",
    "# D_values = [10, 50, 100, 1000] # D\n",
    "accuracy_values = []\n",
    "\n",
    "for LEVELS in D_values:\n",
    "    np.random.seed(42)\n",
    "    codebook = generate_feature_codebook(feature_names, D)\n",
    "    value_level_hvs = generate_value_level_hvs(LEVELS, D)\n",
    "\n",
    "    vectors = [] # encode dataset vectors\n",
    "\n",
    "    for features in dataset:\n",
    "        encoded_hv = encode_feature_vector(features, codebook, value_level_hvs, D, LEVELS)\n",
    "        vectors.append(encoded_hv)\n",
    "\n",
    "    vectors = np.array(vectors)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "\n",
    "    # create weights folder if it doesn't exist\n",
    "    if not os.path.exists(\"weights\"):\n",
    "        os.makedirs(\"weights\")\n",
    "\n",
    "    accuracys = []\n",
    "    for i in range(20):\n",
    "        # train the classifier\n",
    "        class_hvs = train_hd_classifier(vectors, labels, codebook, value_level_hvs, D, LEVELS, epochs=i+1)\n",
    "\n",
    "        # # Save the trained model\n",
    "        # np.save(\"weights/class_hvs.npy\", class_hvs) # save class hypervectors\n",
    "        # np.save(\"weights/codebook.npy\", codebook) # save codebook\n",
    "        # np.save(\"weights/value_level_hvs.npy\", value_level_hvs) # save value level hypervectors\n",
    "        # np.save(\"weights/label_to_id.npy\", label_to_id) # save label_to_id mapping\n",
    "\n",
    "        acc = evaluate_hd(vectors, labels, class_hvs)\n",
    "        accuracys.append(acc*100)\n",
    "        # print(f\"{i+1}: Training accuracy: {acc * 100:.2f}%\")\n",
    "    accuracy_values.append(accuracys)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# --- Plotting the results ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, D in enumerate(D_values):\n",
    "    plt.plot(range(1, 21), accuracy_values[i], label=f'D = {D}')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "# plt.title('Training Accuracy vs Epochs for Different D Values')\n",
    "plt.xticks(range(1, 21))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Calculator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codebook: D*f bytes\n",
    "Class_hv: D*n bytes\n",
    "level_hv: D*l bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Memory Usage: 34.67 KB\n"
     ]
    }
   ],
   "source": [
    "# calculate the memory usage for a given D, LEVELS, number of classes, number of features\n",
    "\n",
    "def calculate_memory_usage(D, LEVELS, num_classes, num_features):\n",
    "    # Memory usage for class hypervectors\n",
    "    class_hvs_memory = D * num_classes * (1) / (1024 ** 1)  # in KB (1 byte per bool)\n",
    "    \n",
    "    # Memory usage for codebook\n",
    "    codebook_memory = D * num_features * (1) / (1024 ** 1) # in KB (1 byte per bool)\n",
    "    \n",
    "    # Memory usage for value level hypervectors\n",
    "    value_level_hvs_memory = LEVELS * D * (1)  / (1024 ** 1)  # in KB (1 byte per bool)\n",
    "    \n",
    "    total_memory = class_hvs_memory + codebook_memory + value_level_hvs_memory\n",
    "    \n",
    "    return total_memory\n",
    "\n",
    "# Example usage\n",
    "D = 250\n",
    "LEVELS = 128\n",
    "num_classes = 5\n",
    "num_features = 9\n",
    "\n",
    "memory_usage = calculate_memory_usage(D, LEVELS, num_classes, num_features)\n",
    "print(f\"Total Memory Usage: {memory_usage:.2f} KB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
